{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off all, we need to read the data using pandas function \".read_csv\".\n",
    "We need to use the read.csv function since the file's extension we'll be using is csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"DataSets/GPA.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, we need to know the shape and the type of data. So, we can get into exploring and analysing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(193, 3)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 193 entries, 0 to 192\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   Gender       189 non-null    object \n",
      " 1   study_hours  189 non-null    float64\n",
      " 2   gpa          189 non-null    float64\n",
      "dtypes: float64(2), object(1)\n",
      "memory usage: 4.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen, this dataset contrains 3 columns followed alongside with 193 rows (entries). 2 of columns (gpa & study_hours) contain numerical values, while 1 of them (gender) contains categorical values. However, 4 of the rows of each column are null values. We can also check for null values that way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender         4\n",
      "study_hours    4\n",
      "gpa            4\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we have 12 null values in total. Thus, we need to check for duplicated values before minding the null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 26 duplicated values which we need to get rid of them before doing anything complicated. We can check for duplicated values seperately by each column alone. However, it is more recommended to check for all once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167, 3)\n"
     ]
    }
   ],
   "source": [
    "df_clean = df.drop_duplicates()\n",
    "print(df_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! As we know, there were 26 duplicated values. Therefore, after dropping them the total entries we have now is 167. If you add them both together, the result will be 193, which is the total of the entries with the duplicated values includer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to null values. Altough, after dropping the duplicates. We need to check for the null values again since some of them could've been duplicateds. (which were dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender         4\n",
      "study_hours    4\n",
      "gpa            4\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_clean.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, so none of them were dropped. Most likely, in categorical null values cases, they are solved by filling the most frequent value with the null value. If we want to check for the most frequent categorical value, we can use that method. Though, we will only be checking for the gender column's entries since it's the only column that contains categorical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender\n",
      "Male      125\n",
      "Female     38\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_clean.Gender.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen, the most frequent categorical value is the gener \"Male\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can replace each categorical null value in the column (\"Gender\") with the most frequent value (\"Male\") using simple imputer from the sklearn module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\htami\\AppData\\Local\\Temp\\ipykernel_16272\\3401033860.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean[[\"Gender\"]] = categorical_imputer.fit_transform(df_nodups[[\"Gender\"]])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer \n",
    "\n",
    "# In gender, there is 4 null values. Solution:\n",
    "\n",
    "from sklearn.impute import SimpleImputer # specialized in handling missing values\n",
    "\n",
    "categorical_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "df_clean[[\"Gender\"]] = categorical_imputer.fit_transform(df_nodups[[\"Gender\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, should be changed. Let's check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gender         0\n",
       "study_hours    4\n",
       "gpa            4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, as expected. All null values have been replaced with the most frequent one. (\"Male\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to focus on the numerical values. (gpa and study_hours column). In numerical values' cases, the null values are most likely replaced by the mean (\"average\") of the data in the column. However, in some cases, that is completely unfair. The second solution is by taking other information from adjacent columns. As we know, the dataset has three columns so each column has 2 sources to grab extra information from. Let's start by comparing the mean of the gpa column with the mean of the gpa column but filtered with the gender and the study_hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5767423312883437\n",
      "3.6174166666666667\n"
     ]
    }
   ],
   "source": [
    "print(df_clean[\"gpa\"].mean())\n",
    "precise_filter = df_clean[(df_clean[\"Gender\"] == \"Male\") & (df_clean[\"study_hours\"] >= 17)]\n",
    "print(precise_filter[\"gpa\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the study hours for 3 for example (just to keep in mind the difference). As well as the gender for male. The differnce in the values are 0.4 only. In such cases, considering the mean is the best solution. Now, we need to do the same thing but with the study hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.588957055214724\n",
      "17.896396396396398\n"
     ]
    }
   ],
   "source": [
    "print(df_clean[\"study_hours\"].mean())\n",
    "precise_filter = df_clean[(df_clean[\"Gender\"] == \"Male\") & (df_clean[\"gpa\"] >= 3.2)]\n",
    "print(precise_filter[\"study_hours\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, as well we set some filters with random values. The difference is almost 0.31, which is not a big deal at all. Thus, we will use the simple imputer with the \"mean\" strategy. As explained why above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\htami\\AppData\\Local\\Temp\\ipykernel_16272\\3008559515.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean[[\"gpa\", \"study_hours\"]] = categorical_imputer.fit_transform(df_clean[[\"gpa\", \"study_hours\"]])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer # specialized in handling missing values\n",
    "\n",
    "categorical_imputer = SimpleImputer(strategy=\"mean\")\n",
    "\n",
    "df_clean[[\"gpa\", \"study_hours\"]] = categorical_imputer.fit_transform(df_clean[[\"gpa\", \"study_hours\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take an eye real quick at the null values now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender         0\n",
      "study_hours    0\n",
      "gpa            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_clean.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! Zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our next step is to actually explore the data. Let's start by using the .descrbie function to get some mathematical interesting information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       study_hours         gpa\n",
      "count   167.000000  167.000000\n",
      "mean     17.588957    3.576742\n",
      "std      11.396195    0.286676\n",
      "min       2.000000    2.600000\n",
      "25%      10.000000    3.400000\n",
      "50%      15.000000    3.600000\n",
      "75%      20.000000    3.800000\n",
      "max      69.000000    4.300000\n"
     ]
    }
   ],
   "source": [
    "print(df_clean.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The describe function is only specialized in numerical values. That's why the gender column isn't there (it's a categorical column). Let's start by exploring the study_hours column, it basically indicates the study hours students spent studying. The total count of the study hours columns entries is 167. The average of the study hours per studern is about 18 hours which is realistic for a normal/ good student (averages about 2.5 hours per day), if we're considering that these are their study hours per week which is the most realistic option. The minmum hours a student has spent is 2 hours, and in return he got a 2.6 grade average (gpa). The highest amount of hours a student has studies is 69 hours, has an average of almost 10 hours per day. (which indicates that the student is extremely dedicated and determined) In return, he got 4.3 which is considered an outlier grade (will get to that topic later). There is also some interesting information such as what does 25, 50, 75% of the students study. As well, as the STR (short tandem repeats) for the students' study hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, the gpa (stands for \"grade per point\") column, which indicates the total grade of students. The process is similar to the study hours column. First of all, the count of the entire rows (entries) equals 167 (which makes sense, supposed to be smilar to the other entries in other columns). The average of the grades is 3.5 which is something impressive since the full mark is most likely 4. The maximum grade a student has ever got is 4.3 (an outlier value). The student whose mark is 4.3 has studied 69 hours, averaging almost 10 hours per day. There is also as well some interesting information such as what does 25, 50, 75% of the students study. As well, as the STR (short tandem repeats) for the students' gpa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring Outliers. Outliers are considered exceptions which are most likely inputting mistakes. Outliers are explored theoretically, and by common sense. For example, the 4.3 grade. In gpa ratious, there isn't any 4.3 grades. It must be an integer number. There is only 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Gender  study_hours  gpa\n",
      "79  Female         10.0  4.3\n"
     ]
    }
   ],
   "source": [
    "filter_outliers = df[(df[\"gpa\"] > 4) | (df[\"gpa\"] < 0)]\n",
    "\n",
    "print(filter_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Outliers are considered exceptions which are most likely inputting mistakes. Outliers are explored theoretically, and by common sense (must use filters). In this dataset, there is only one outlier value which is \"4.3\". Since in this dataset, full marks are 4. Now, it comes down to 2 options: The original mark is either 3.4 (highest chance). Or, that student, got extra marks. However, what's the chance of getting a 0.3 bonus, or getting a 1+ bonus and losing 0.7 a mark? In such cases, it's recommended to contact the original source of the dataset. Secondly, The outlier data needs to be replaced with the option that is valid and makes sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean[\"gpa\"].replace(4.3, 3.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done! This was the process of exploring the data, containing the most important and useful methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
